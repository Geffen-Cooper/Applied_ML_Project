{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting external features from Google Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import responses\n",
    "import googlemaps\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load API key\n",
    "see https://developers.google.com/maps/get-started for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_api_key():\n",
    "    with open('config.yml', 'r') as stream:\n",
    "        try:\n",
    "            config = yaml.safe_load(stream)\n",
    "            api_key = config['api_key']\n",
    "            return api_key\n",
    "        except yaml.YAMLError as e:\n",
    "            print(f\"Error loading YAML file: {e}\")\n",
    "            return None\n",
    "\n",
    "# Example usage\n",
    "api_key = load_api_key()\n",
    "client = googlemaps.Client(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/train.csv')\n",
    "df_test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "separate features from labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate labels from features\n",
    "full_training_label_cols = ['TotalTimeStopped_p20','TotalTimeStopped_p40','TotalTimeStopped_p50',\n",
    "                       'TotalTimeStopped_p60','TotalTimeStopped_p80',\n",
    "                       'TimeFromFirstStop_p20','TimeFromFirstStop_p40','TimeFromFirstStop_p50',\n",
    "                       'TimeFromFirstStop_p60','TimeFromFirstStop_p80',\n",
    "                       'DistanceToFirstStop_p20','DistanceToFirstStop_p40','DistanceToFirstStop_p50',\n",
    "                       'DistanceToFirstStop_p60','DistanceToFirstStop_p80']\n",
    "rel_training_label_cols = ['IntersectionId','TotalTimeStopped_p20','TotalTimeStopped_p50','TotalTimeStopped_p80',\n",
    "                       'DistanceToFirstStop_p20','DistanceToFirstStop_p50','DistanceToFirstStop_p80']\n",
    "\n",
    "\n",
    "df_train_y = df_train[rel_training_label_cols]\n",
    "\n",
    "# drop columns not needed\n",
    "df_test_X = df_test.drop(columns=['RowId'])\n",
    "df_train_X = df_train.drop(columns=['RowId']+full_training_label_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "separate by city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['Atlanta','Boston','Chicago','Philadelphia']\n",
    "city_training_datasets_X = {}\n",
    "city_training_datasets_y = {}\n",
    "city_testing_datasets_X = {}\n",
    "\n",
    "# training\n",
    "for city in cities:\n",
    "    # get rows for this city\n",
    "    city_idxs = (df_train_X['City'] == city).values.nonzero()[0]\n",
    "\n",
    "    # get X (data) and y (labels)\n",
    "    city_training_datasets_X[city] = df_train_X.loc[city_idxs]\n",
    "    city_training_datasets_y[city] = df_train_y.loc[city_idxs]\n",
    "\n",
    "    # drop the city column from X (data)\n",
    "    city_training_datasets_X[city] = city_training_datasets_X[city].drop(columns=['City'])\n",
    "\n",
    "# testing\n",
    "for city in cities:\n",
    "    # get rows for this city\n",
    "    city_idxs = (df_test_X['City'] == city).values.nonzero()[0]\n",
    "\n",
    "    # get X (data)\n",
    "    city_testing_datasets_X[city] = df_test_X.loc[city_idxs]\n",
    "\n",
    "    # drop the city column from X (data)\n",
    "    city_testing_datasets_X[city] = city_testing_datasets_X[city].drop(columns=['City'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to get data from google maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elev(long_lat_tuples):\n",
    "    elevs = np.zeros(len(long_lat_tuples),dtype=float)\n",
    "    start = 0\n",
    "    while start+32 < len(elevs):\n",
    "        result = client.elevation(long_lat_tuples[start:start+32])\n",
    "        result = [sub_res['elevation'] for sub_res in result]\n",
    "        elevs[start:start+32] = result[:]\n",
    "        start += 32\n",
    "    result = client.elevation(long_lat_tuples[start:len(elevs)])\n",
    "    result = np.array([sub_res['elevation'] for sub_res in result])\n",
    "    elevs[start:len(elevs)] = result[:]\n",
    "\n",
    "    return elevs\n",
    "\n",
    "def get_dist(long_lat_tuples,destination):\n",
    "    dist_meters = np.zeros(len(long_lat_tuples),dtype=float)\n",
    "    start = 0\n",
    "    while start+16 < len(dist_meters):\n",
    "        result = client.distance_matrix(long_lat_tuples[start:start+16],destination)\n",
    "        result = result['rows']\n",
    "        result = [sub_res['elements'][0]['distance']['value'] for sub_res in result]\n",
    "        dist_meters[start:start+16] = result[:]\n",
    "        start += 16\n",
    "    result = client.distance_matrix(long_lat_tuples[start:len(dist_meters)],destination)\n",
    "    result = result['rows']\n",
    "    result = [sub_res['elements'][0]['distance']['value'] for sub_res in result]\n",
    "    dist_meters[start:len(dist_meters)] = result[:]\n",
    "\n",
    "    return dist_meters\n",
    "\n",
    "def get_num_places_50(long_lat_tuples):\n",
    "    num_places_50 = np.zeros(len(long_lat_tuples),dtype=float)\n",
    "    for i,place in enumerate(tqdm(long_lat_tuples)):\n",
    "        result = client.places_nearby(location=place,radius=50)\n",
    "        result = str(result['results']).count('geometry')\n",
    "        num_places_50[i] = result\n",
    "\n",
    "    return num_places_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_center = pd.DataFrame({\"Atlanta\":[33.753746, -84.386330],\n",
    "                             \"Boston\":[42.361145, -71.057083],\n",
    "                             \"Chicago\":[41.881832, -87.623177],\n",
    "                             \"Philadelphia\":[39.952583, -75.165222]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get the data using the functions from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlanta\n",
      "elevation\n",
      "distance\n",
      "num_places\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 377/377 [01:06<00:00,  5.67it/s]\n",
      "100%|██████████| 468/468 [00:51<00:00,  9.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston\n",
      "elevation\n",
      "distance\n",
      "num_places\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 975/975 [03:05<00:00,  5.26it/s]\n",
      "100%|██████████| 1192/1192 [02:56<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chicago\n",
      "elevation\n",
      "distance\n",
      "num_places\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2135/2135 [07:04<00:00,  5.03it/s]\n",
      "100%|██████████| 2571/2571 [07:00<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Philadelphia\n",
      "elevation\n",
      "distance\n",
      "num_places\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1318/1318 [04:00<00:00,  5.49it/s]\n",
      "100%|██████████| 1716/1716 [03:47<00:00,  7.55it/s]\n"
     ]
    }
   ],
   "source": [
    "for city in cities:\n",
    "    print(city)\n",
    "    grouped_train = city_training_datasets_X[city].groupby('IntersectionId').agg({'Latitude': 'first','Longitude': 'first'}).reset_index()\n",
    "    grouped_test = city_testing_datasets_X[city].groupby('IntersectionId').agg({'Latitude': 'first','Longitude': 'first'}).reset_index()\n",
    "\n",
    "    train_long_lat = grouped_train[['Latitude','Longitude']].values\n",
    "    test_long_lat = grouped_test[['Latitude','Longitude']].values\n",
    "\n",
    "    train_long_lat_tuples = [tuple(long_lat) for long_lat in train_long_lat]\n",
    "    test_long_lat_tuples = [tuple(long_lat) for long_lat in test_long_lat]\n",
    "\n",
    "    print('elevation')\n",
    "    grouped_train['elevation'] = get_elev(train_long_lat_tuples)\n",
    "    grouped_test['elevation'] = get_elev(test_long_lat_tuples)\n",
    "\n",
    "    print('distance')\n",
    "    grouped_train['cent_dist'] = get_dist(train_long_lat_tuples,[tuple(df_center[city])])\n",
    "    grouped_test['cent_dist'] = get_dist(test_long_lat_tuples,[tuple(df_center[city])])\n",
    "\n",
    "    print('num_places')\n",
    "    grouped_train['num_places_50'] = get_num_places_50(train_long_lat_tuples)\n",
    "    grouped_test['num_places_50'] = get_num_places_50(test_long_lat_tuples)\n",
    "\n",
    "    city_training_datasets_X[city] = pd.merge(city_training_datasets_X[city], grouped_train, on='IntersectionId', how='left')\n",
    "    city_testing_datasets_X[city] = pd.merge(city_testing_datasets_X[city], grouped_test, on='IntersectionId', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data to disk. Need to upload to google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/gmaps_training_X.pickle', 'wb') as handle:\n",
    "    pickle.dump(city_training_datasets_X, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('../data/gmaps_testing_X.pickle', 'wb') as handle:\n",
    "    pickle.dump(city_testing_datasets_X, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
